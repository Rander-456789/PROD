{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d568452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "������ ������� ��࠭��: 1251\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.15.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\programdata\\anaconda3\\lib\\site-packages (from smart_open>=1.8.1->gensim) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e23ef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from gensim.models import FastText\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import joblib\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb80555",
   "metadata": {},
   "source": [
    "ПС я не стал оставлять в финальной работе различный просмотр статистик, графики, хэды и дт."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75968c8e",
   "metadata": {},
   "source": [
    "# Объединение таблиц"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2b794f",
   "metadata": {},
   "source": [
    "Загрузка таблиц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e611f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_csv('customers.csv')\n",
    "merchants = pd.read_csv('merchants.csv')\n",
    "receipts = pd.read_csv('receipts.csv')\n",
    "terminals = pd.read_csv('terminals.csv')\n",
    "transactions = pd.read_csv('transactions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4976a6c4",
   "metadata": {},
   "source": [
    "Обработка receipts - так касумма по транзакции уже имеется, цену отдельных товаров можно не брать (по моему предположению показатели из серии разброс в чеке, минимальный или макимальный элемент не должны сильно влиять на модель, но я могу и ошибаться Y(^_^)Y ), а названия обьединил в одну строчку для будующего TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07725ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts = receipts.dropna(subset=[\"item_name\"])\n",
    "\n",
    "receipts_agg = (receipts.groupby(\"transaction_id\").agg(\n",
    "        receipt_text=(\"item_name\", lambda x: \" \".join(x.astype(str))), # эту конструкцию по запросу написал генеративный ии\n",
    "        receipt_prices=(\"item_price\", lambda x: list(x.astype(float)))\n",
    "    ).reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a372323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>receipt_text</th>\n",
       "      <th>receipt_prices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TX00000000</td>\n",
       "      <td>unit part</td>\n",
       "      <td>[1.81, 48.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TX00000001</td>\n",
       "      <td>thing</td>\n",
       "      <td>[9.87]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TX00000002</td>\n",
       "      <td>Ahsen Sernade item item</td>\n",
       "      <td>[80.01, 78.5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TX00000003</td>\n",
       "      <td>Cacsae item Elsewhee</td>\n",
       "      <td>[36.83]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TX00000004</td>\n",
       "      <td>part aRivulet stuff dRiviulet Suafron piece</td>\n",
       "      <td>[27.37, 64.59, 5.96]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>TX00001495</td>\n",
       "      <td>Moonthreaed thing</td>\n",
       "      <td>[10.76]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>TX00001496</td>\n",
       "      <td>thing part</td>\n",
       "      <td>[45.38]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>TX00001497</td>\n",
       "      <td>thing object</td>\n",
       "      <td>[17.17, 81.49]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>TX00001498</td>\n",
       "      <td>Auorelic uEmbrace oonthread thing Gulow thing</td>\n",
       "      <td>[62.41, 32.47, 37.63, 8.87]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>TX00001499</td>\n",
       "      <td>Aurloic thing Gildrain thing Lenternwisp unit</td>\n",
       "      <td>[82.25, 30.16, 8.56]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1432 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     transaction_id                                   receipt_text  \\\n",
       "0        TX00000000                                      unit part   \n",
       "1        TX00000001                                          thing   \n",
       "2        TX00000002                        Ahsen Sernade item item   \n",
       "3        TX00000003                           Cacsae item Elsewhee   \n",
       "4        TX00000004    part aRivulet stuff dRiviulet Suafron piece   \n",
       "...             ...                                            ...   \n",
       "1427     TX00001495                              Moonthreaed thing   \n",
       "1428     TX00001496                                     thing part   \n",
       "1429     TX00001497                                   thing object   \n",
       "1430     TX00001498  Auorelic uEmbrace oonthread thing Gulow thing   \n",
       "1431     TX00001499  Aurloic thing Gildrain thing Lenternwisp unit   \n",
       "\n",
       "                   receipt_prices  \n",
       "0                   [1.81, 48.01]  \n",
       "1                          [9.87]  \n",
       "2                   [80.01, 78.5]  \n",
       "3                         [36.83]  \n",
       "4            [27.37, 64.59, 5.96]  \n",
       "...                           ...  \n",
       "1427                      [10.76]  \n",
       "1428                      [45.38]  \n",
       "1429               [17.17, 81.49]  \n",
       "1430  [62.41, 32.47, 37.63, 8.87]  \n",
       "1431         [82.25, 30.16, 8.56]  \n",
       "\n",
       "[1432 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "receipts_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c70afef",
   "metadata": {},
   "source": [
    "Обработка terminals - пропуски (их всего 2) замени на пустую строку, опять же во имя TF-IDF. обьеденил их с merchants и выбросил ненужные так как есть критерии прода(продакшена) столбцы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85ae37a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merchants = merchants.drop(['merchant_name'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00ac7b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "terminals['terminal_description'] = terminals['terminal_description'].fillna(\"\")\n",
    "terminals_agg = (terminals.merge(merchants, on=\"merchant_id\"))\n",
    "terminals_agg = terminals_agg[['terminal_id', 'merchant_id', 'terminal_description', 'merchant_city']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb44816",
   "metadata": {},
   "source": [
    "Далее 1 блока работы с transactions и последние 3 это объединение таблиц, замена пропусков на ничего во имя TF-IDF. ну и удаление бесполезных айдишников и парочки признаков которые я не доглядел. terminal_id используем в будущем для того что бы на train были другие терминалы в будующем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c942736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_agg = transactions[['transaction_id', 'terminal_id', 'customer_id', 'amount', 'item_count', 'true_mcc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "965c65b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f = (transactions_agg\n",
    "    .merge(terminals_agg, on=\"terminal_id\")\n",
    "    .merge(customers, on=\"customer_id\")\n",
    "    .merge(receipts_agg, on=\"transaction_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "665204d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f[\"receipt_text\"] = data_f[\"receipt_text\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34d45cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idshniki = [\"transaction_id\", \"merchant_id\", \"customer_id\", 'customer_segment']\n",
    "data = data_f.drop(columns=idshniki)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7393ca",
   "metadata": {},
   "source": [
    "Итог первого шага - обьединение всех таблиц и первое знакомство с данными. Заняло кстати у меня это +- 2 часа. Ибо я сразу не разобрался что на проде будут только эти данные и реально сидел отбирал что подойдет."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f1467a",
   "metadata": {},
   "source": [
    "# Базовое решение, на основе которого будем строить оценку новых"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b61da77",
   "metadata": {},
   "source": [
    "Функция переводит все в строчные, убирает шумные символы if есть и убирает сдвоенные пробелы. Это все на всякий случай и для того что бы потом TF-IDF корректно работал."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c04cfa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = str(text).lower() \n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "data['terminal_description'] = data['terminal_description'].apply(normalize_text)\n",
    "data['merchant_city'] = data['merchant_city'].apply(normalize_text)\n",
    "data['receipt_text'] = data['receipt_text'].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d4727f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"full_text\"] = (data[\"receipt_text\"] + \" \" + data[\"terminal_description\"] + \" \" + data[\"merchant_city\"])\n",
    "data[\"full_text\"] = data[\"full_text\"].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54edacfe",
   "metadata": {},
   "source": [
    "Осевое решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94a70d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['true_mcc']\n",
    "x = data[['amount', 'item_count', 'full_text']]\n",
    "groups = data['terminal_id']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state= 456, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7981ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"txt\", TfidfVectorizer(\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=2, # выбрано 2 для того что бы опечатки не были выбраны как признак, прописываю так как в задании был акцент на очепятки\n",
    "                max_df=0.9,\n",
    "                sublinear_tf=True), # что бы повторения не завышали веса\n",
    "            \"full_text\"\n",
    "        ),\n",
    "        (\"num\", StandardScaler(),[\"amount\", \"item_count\"]) # так как далее прменим линейку\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "\n",
    "pepa = Pipeline( steps=[(\"prep\", preprocessor), (\"clf\", model)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3945415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.7909\n"
     ]
    }
   ],
   "source": [
    "pepa.fit(x_train, y_train)\n",
    "\n",
    "y_pred = pepa.predict(x_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Baseline accuracy:\", round(acc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fefddb",
   "metadata": {},
   "source": [
    "# Улучшения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df85ada",
   "metadata": {},
   "source": [
    "## Первое улучшение это попытка предотвратить leakage, так как на проде будут новые терминалы, то "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94564a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1,test_size=0.2, random_state=11745)\n",
    "\n",
    "train_id, test_id = next(gss.split(x, y, groups=data['terminal_id']))#теперь в тесте будут только новые терминалы\n",
    "\n",
    "x_train = x.iloc[train_id]\n",
    "x_test  = x.iloc[test_id]\n",
    "y_train = y.iloc[train_id]\n",
    "y_test  = y.iloc[test_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1e2e478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.21\n"
     ]
    }
   ],
   "source": [
    "# analogichnо\n",
    "\n",
    "model = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "pepa = Pipeline( steps=[(\"prep\", preprocessor), (\"clf\", model)])\n",
    "\n",
    "pepa.fit(x_train, y_train)\n",
    "y_pred = pepa.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Baseline accuracy:\", round(acc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1821ea64",
   "metadata": {},
   "source": [
    "У меня если честно сердце здесь заболело, но это правдивая метрика, так как мы фактически еще и не решали"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eec237b",
   "metadata": {},
   "source": [
    "## Несколько статистик из товаров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "155723a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_features(prices, amount):\n",
    "    if not prices: return {\n",
    "            \"min\": 0,\n",
    "            \"sum_ratio\": 0\n",
    "        }\n",
    "\n",
    "    prices = np.array(prices)\n",
    "    return {\n",
    "        \"min\": prices.min(),\n",
    "        \"sum_ratio\": prices.sum() / (amount + 0.00001) # xnj ,s yt gjltkbnm yf 0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce8f18fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_feats = data.apply(lambda row: price_features(row[\"receipt_prices\"], row[\"amount\"]), axis=1,result_type=\"expand\")\n",
    "data = pd.concat([data, price_feats], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3dbe1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop(['terminal_id', 'true_mcc', 'terminal_description', 'merchant_city', 'receipt_text', 'receipt_prices'], axis = 1) # меняем так как появились новые столбцы, остальное как было\n",
    "\n",
    "x_train = x.iloc[train_id]\n",
    "x_test  = x.iloc[test_id]\n",
    "y_train = y.iloc[train_id]\n",
    "y_test  = y.iloc[test_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b506b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.2135\n"
     ]
    }
   ],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"txt\", TfidfVectorizer(\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=2,\n",
    "                max_df=0.9,\n",
    "                sublinear_tf=True),\n",
    "            \"full_text\"\n",
    "        ),\n",
    "        (\"num\", StandardScaler(), [\"amount\", \"item_count\"]), \n",
    "        (\"num_raw\", \"passthrough\", [\"min\", \"sum_ratio\"]) # добавили новые признаки\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "model = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
    "pepa = Pipeline( steps=[(\"prep\", preprocessor), (\"clf\", model)])\n",
    "\n",
    "pepa.fit(x_train, y_train)\n",
    "y_pred = pepa.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Baseline accuracy:\", round(acc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da878d65",
   "metadata": {},
   "source": [
    "## Борба с опечатками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77c0cef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# делю текст на описание товаров(врорде как реально важен) и доп информацию\n",
    "data[\"context_text\"] = (data[\"terminal_description\"] + \" \" + data[\"merchant_city\"]).apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a79c0920",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop(['terminal_id', 'true_mcc', 'terminal_description', 'merchant_city', 'receipt_prices', 'full_text'], axis = 1)# вновь новые колонки\n",
    "\n",
    "x_train = x.iloc[train_id]\n",
    "x_test  = x.iloc[test_id]\n",
    "y_train = y.iloc[train_id]\n",
    "y_test  = y.iloc[test_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92be723b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m model = LogisticRegression(max_iter=\u001b[32m2000\u001b[39m, class_weight=\u001b[33m\"\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m pepa = Pipeline( steps=[(\u001b[33m\"\u001b[39m\u001b[33mprep\u001b[39m\u001b[33m\"\u001b[39m, preprocessor), (\u001b[33m\"\u001b[39m\u001b[33mclf\u001b[39m\u001b[33m\"\u001b[39m, model)])\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mpepa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m y_pred = pepa.predict(x_test)\n\u001b[32m     37\u001b[39m acc = accuracy_score(y_test, y_pred)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:662\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    657\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    658\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    659\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    660\u001b[39m             all_params=params,\n\u001b[32m    661\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1350\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1347\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1348\u001b[39m     n_threads = \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1350\u001b[39m fold_coefs_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1353\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1364\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1368\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m fold_coefs_, _, n_iter_ = \u001b[38;5;28mzip\u001b[39m(*fold_coefs_)\n\u001b[32m   1376\u001b[39m \u001b[38;5;28mself\u001b[39m.n_iter_ = np.asarray(n_iter_, dtype=np.int32)[:, \u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1916\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1917\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1921\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1922\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1847\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1849\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     config = {}\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:451\u001b[39m, in \u001b[36m_logistic_regression_path\u001b[39m\u001b[34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[39m\n\u001b[32m    447\u001b[39m l2_reg_strength = \u001b[32m1.0\u001b[39m / (C * sw_sum)\n\u001b[32m    448\u001b[39m iprint = [-\u001b[32m1\u001b[39m, \u001b[32m50\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m100\u001b[39m, \u001b[32m101\u001b[39m][\n\u001b[32m    449\u001b[39m     np.searchsorted(np.array([\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m]), verbose)\n\u001b[32m    450\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m opt_res = \u001b[43moptimize\u001b[49m\u001b[43m.\u001b[49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mL-BFGS-B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxiter\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# default is 20\u001b[39;49;00m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43miprint\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgtol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mftol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m n_iter_i = _check_optimize_result(\n\u001b[32m    466\u001b[39m     solver,\n\u001b[32m    467\u001b[39m     opt_res,\n\u001b[32m    468\u001b[39m     max_iter,\n\u001b[32m    469\u001b[39m     extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[32m    470\u001b[39m )\n\u001b[32m    471\u001b[39m w0, loss = opt_res.x, opt_res.fun\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:738\u001b[39m, in \u001b[36mminimize\u001b[39m\u001b[34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[39m\n\u001b[32m    735\u001b[39m     res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[32m    736\u001b[39m                              **options)\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33ml-bfgs-b\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m738\u001b[39m     res = \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33mtnc\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    741\u001b[39m     res = _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n\u001b[32m    742\u001b[39m                         **options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:441\u001b[39m, in \u001b[36m_minimize_lbfgsb\u001b[39m\u001b[34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[39m\n\u001b[32m    433\u001b[39m _lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr, pgtol, wa,\n\u001b[32m    434\u001b[39m                iwa, task, lsave, isave, dsave, maxls, ln_task)\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m3\u001b[39m:\n\u001b[32m    437\u001b[39m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[32m    438\u001b[39m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[32m    439\u001b[39m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[32m    440\u001b[39m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m     f, g = \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m1\u001b[39m:\n\u001b[32m    443\u001b[39m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[32m    444\u001b[39m     n_iterations += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:341\u001b[39m, in \u001b[36mScalarFunction.fun_and_grad\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    338\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_hess()\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.H\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfun_and_grad\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.array_equal(x, \u001b[38;5;28mself\u001b[39m.x):\n\u001b[32m    343\u001b[39m         \u001b[38;5;28mself\u001b[39m._update_x(x)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#переконопатили всю архитектуру\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"receipt_txt\", TfidfVectorizer(\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=2,\n",
    "                max_df=0.9,\n",
    "                sublinear_tf=True\n",
    "            ), \"receipt_text\"\n",
    "        ),\n",
    "        (\"receipt_char\", TfidfVectorizer( # эта часть призвана помочь с опечатками блягодоря analyzer=\"char\" и ngram_range=(3, 5)\n",
    "                analyzer=\"char\",\n",
    "                ngram_range=(3, 5),\n",
    "                min_df=2,\n",
    "                max_df=0.9,\n",
    "                sublinear_tf=True\n",
    "            ), \"receipt_text\"\n",
    "        ),\n",
    "        (\"context_txt\", TfidfVectorizer(\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=2,\n",
    "                max_df=0.9\n",
    "            ), \"context_text\"\n",
    "        ),\n",
    "        (\"num\", StandardScaler(), [\"amount\", \"item_count\"]),\n",
    "        (\"num_raw\", \"passthrough\", [\"min\", \"sum_ratio\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "model = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
    "pepa = Pipeline( steps=[(\"prep\", preprocessor), (\"clf\", model)])\n",
    "\n",
    "pepa.fit(x_train, y_train)\n",
    "y_pred = pepa.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Baseline accuracy:\", round(acc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32417a3b",
   "metadata": {},
   "source": [
    "Как итог видим сильный буст, так как даже в задании прописано что опечаток будет много)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c73584e",
   "metadata": {},
   "source": [
    "## Замена алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3393df23",
   "metadata": {},
   "source": [
    "естественно остановится на линейке мы не можем, протестируем еще парочку"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff6719b",
   "metadata": {},
   "source": [
    "Так как данные мы не меняем то preprocessor а так же все train/test таблицы берем из имеющихся уже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae81867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg 0.4413\n",
      "sgd 0.1032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linsvc 0.5374\n",
      "pa 0.2847\n"
     ]
    }
   ],
   "source": [
    "# Для скорости делал через нейронку\n",
    "models = {\n",
    "    \"logreg\": LogisticRegression(max_iter=3000, class_weight=\"balanced\"),\n",
    "    \"sgd\": SGDClassifier(loss=\"log_loss\", alpha=1e-5, max_iter=3000, class_weight=\"balanced\"),\n",
    "    \"linsvc\": CalibratedClassifierCV(LinearSVC(class_weight=\"balanced\"),  cv=10),\n",
    "    \"pa\": PassiveAggressiveClassifier(max_iter=2000, class_weight=\"balanced\")\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline([(\"prep\", preprocessor), (\"clf\", model)])\n",
    "    pipe.fit(x_train, y_train)\n",
    "    acc = accuracy_score(y_test, pipe.predict(x_test))\n",
    "    print(name, round(acc, 4))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fd114d",
   "metadata": {},
   "source": [
    "Как итог лучшим показал себя LinearSVC(так как данных мало и самые главные в тексте), на нем и остановимся"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6caaee6",
   "metadata": {},
   "source": [
    "## Реализация fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "757fbe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['l2_rec'] = data['receipt_text'].apply(lambda x:len(x))\n",
    "\n",
    "x = data.drop(['terminal_id', 'true_mcc', 'terminal_description', 'merchant_city', 'receipt_prices', 'full_text'], axis = 1)# вновь новые колонки\n",
    "\n",
    "x_train = x.iloc[train_id]\n",
    "x_test  = x.iloc[test_id]\n",
    "y_train = y.iloc[train_id]\n",
    "y_test  = y.iloc[test_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788897ff",
   "metadata": {},
   "source": [
    "С помощью библиотеки обучаем модель векторизировать слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f5de4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuprus = (x_train[\"receipt_text\"].tolist() + x_train[\"context_text\"].tolist())\n",
    "\n",
    "model_fst = FastText(vector_size=100, window=5, min_count=1, sg=1, workers=4)\n",
    "\n",
    "model_fst.build_vocab(cuprus)\n",
    "model_fst.train(cuprus, total_examples=len(cuprus), epochs=10)\n",
    "\n",
    "model_fst =  model_fst.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f4112a",
   "metadata": {},
   "source": [
    "подсчет частот"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d26eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter()\n",
    "for text in cuprus:\n",
    "    word_freq.update(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac5b719",
   "metadata": {},
   "source": [
    "Класс который я списал полностью, так как +- это было уже реализовано в статье которую я читал + GPT помог переделать под мою задачу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8eb48c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextWeightedMean(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, wv, dim, word_freq):\n",
    "        self.wv = wv\n",
    "        self.dim = dim\n",
    "        self.word_freq = word_freq\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        n = len(X)\n",
    "        result = np.zeros((n, self.dim), dtype=np.float32)\n",
    "\n",
    "        for i, text in enumerate(X):\n",
    "            tokens = str(text).split()\n",
    "            weight_sum = 0\n",
    "\n",
    "            for tok in tokens:\n",
    "                if tok in self.wv:\n",
    "                    w = 1 / (1 + self.word_freq.get(tok, 0))\n",
    "                    result[i] += self.wv[tok][:self.dim] * w\n",
    "                    weight_sum += w\n",
    "\n",
    "            if weight_sum > 0:\n",
    "                result[i] /= weight_sum\n",
    "\n",
    "        return result\n",
    "    \n",
    "class FastTextWeightedMeanScaled(FastTextWeightedMean):\n",
    "    def __init__(self, wv, dim, word_freq, scale=1.0):\n",
    "        super().__init__(wv, dim, word_freq)\n",
    "        self.scale = scale\n",
    "    def transform(self, X):\n",
    "        return super().transform(X) * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b65722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.7758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"receipt_char\", TfidfVectorizer(\n",
    "                analyzer=\"char\",\n",
    "                ngram_range=(3, 5),\n",
    "                min_df=2,\n",
    "                max_df=0.9,\n",
    "                sublinear_tf=True\n",
    "            ), \"receipt_text\"\n",
    "        ),\n",
    "        (\"ft_receipt\", FastTextWeightedMeanScaled(model_fst, 50, word_freq, scale=1.5), \"receipt_text\"),\n",
    "        (\"ft_context\", FastTextWeightedMeanScaled(model_fst, 50, word_freq, scale=0.7), \"context_text\"),\n",
    "        (\"num\", StandardScaler(), [\"amount\", \"item_count\", \"min\", \"sum_ratio\",  \"l2_rec\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = CalibratedClassifierCV(LinearSVC(class_weight=\"balanced\"),  cv=10)\n",
    "pepa = Pipeline( steps=[(\"prep\", preprocessor), (\"clf\", model)])\n",
    "\n",
    "pepa.fit(x_train, y_train)\n",
    "y_pred = pepa.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Baseline accuracy:\", round(acc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affd19d1",
   "metadata": {},
   "source": [
    "Как итог, интеграция FastText и удаление лишних параметров далли заметнейший буст, а так как дедлайн завтра, то это финальная версия."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86bde1f",
   "metadata": {},
   "source": [
    "# Финальная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745a8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_csv('customers.csv')\n",
    "merchants = pd.read_csv('merchants.csv')\n",
    "receipts = pd.read_csv('receipts.csv')\n",
    "terminals = pd.read_csv('terminals.csv')\n",
    "transactions = pd.read_csv('transactions.csv')\n",
    "receipts = receipts.dropna(subset=[\"item_name\"])\n",
    "\n",
    "receipts_agg = (receipts.groupby(\"transaction_id\").agg(\n",
    "        receipt_text=(\"item_name\", lambda x: \" \".join(x.astype(str))),\n",
    "        receipt_prices=(\"item_price\", lambda x: list(x.astype(float)))\n",
    "    ).reset_index()\n",
    ")\n",
    "\n",
    "merchants = merchants.drop(['merchant_name'], axis = 1)\n",
    "\n",
    "terminals['terminal_description'] = terminals['terminal_description'].fillna(\"\")\n",
    "terminals_agg = (terminals.merge(merchants, on=\"merchant_id\"))\n",
    "terminals_agg = terminals_agg[['terminal_id', 'merchant_id', 'terminal_description', 'merchant_city']]\n",
    "transactions_agg = transactions[['transaction_id', 'terminal_id', 'customer_id', 'amount', 'item_count', 'true_mcc']]\n",
    "\n",
    "data_f = (transactions_agg\n",
    "    .merge(terminals_agg, on=\"terminal_id\")\n",
    "    .merge(customers, on=\"customer_id\")\n",
    "    .merge(receipts_agg, on=\"transaction_id\")\n",
    ")\n",
    "\n",
    "data_f[\"receipt_text\"] = data_f[\"receipt_text\"].fillna(\"\")\n",
    "idshniki = [\"transaction_id\", \"merchant_id\", \"customer_id\", 'customer_segment']\n",
    "data = data_f.drop(columns=idshniki)\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = str(text).lower() \n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "data['terminal_description'] = data['terminal_description'].apply(normalize_text)\n",
    "data['merchant_city'] = data['merchant_city'].apply(normalize_text)\n",
    "data['receipt_text'] = data['receipt_text'].apply(normalize_text)\n",
    "\n",
    "def price_features(prices, amount):\n",
    "    if not prices: return {\n",
    "            \"min\": 0,\n",
    "            \"sum_ratio\": 0\n",
    "        }\n",
    "\n",
    "    prices = np.array(prices)\n",
    "    return {\n",
    "        \"min\": prices.min(),\n",
    "        \"sum_ratio\": prices.sum() / (amount + 0.00001)\n",
    "    }\n",
    "\n",
    "price_feats = data.apply(lambda row: price_features(row[\"receipt_prices\"], row[\"amount\"]), axis=1,result_type=\"expand\")\n",
    "data = pd.concat([data, price_feats], axis=1)\n",
    "\n",
    "data[\"context_text\"] = (data[\"terminal_description\"] + \" \" + data[\"merchant_city\"]).apply(normalize_text)\n",
    "data['l2_rec'] = data['receipt_text'].apply(lambda x:len(x))\n",
    "\n",
    "x = data.drop(['terminal_id', 'terminal_description', 'merchant_city', 'receipt_prices'], axis = 1)\n",
    "y = data['true_mcc']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6a7451",
   "metadata": {},
   "source": [
    "Проблема fast Text была в медленности, я не укладывался в лимиты, а точности они дают не много, так что без них теперь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d8b4e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"receipt_char\", TfidfVectorizer(\n",
    "                analyzer=\"char\",\n",
    "                ngram_range=(3, 5),\n",
    "                min_df=2,\n",
    "                max_df=0.9,\n",
    "                sublinear_tf=True\n",
    "            ), \"receipt_text\"\n",
    "        ),\n",
    "        (\"num\", StandardScaler(), [\"amount\", \"item_count\", \"min\", \"sum_ratio\",  \"l2_rec\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = CalibratedClassifierCV(LinearSVC(class_weight=\"balanced\"),  cv=10)\n",
    "pepa = Pipeline( steps=[(\"prep\", preprocessor), (\"clf\", model)])\n",
    "\n",
    "pepa.fit(x, y)\n",
    "joblib.dump(pepa, \"model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8ae84e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
